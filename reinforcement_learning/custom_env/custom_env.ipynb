{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "cf10a702",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Imports\n",
    "import gym\n",
    "from gym import spaces\n",
    "import numpy as np\n",
    "import io\n",
    "from IPython.core.debugger import set_trace\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "90ca5627",
   "metadata": {},
   "outputs": [],
   "source": [
    "class env(gym.Env):\n",
    "\n",
    "    metadata = {'render_modes': ['human', 'ansi']}\n",
    "    \n",
    "    def __init__(self, shape = [4,4]):\n",
    "        if not isinstance(shape, (list, tuple)) or len(shape) < 2:\n",
    "            raise ValueError('shape must be a list or a tuple with a minimum length of 2')\n",
    "        \n",
    "        self.shape = shape\n",
    "        self.num_states = np.prod(shape)\n",
    "\n",
    "        self.num_actions = 4  #gym.spaces.Discrete(4)  # states = 0, 1, 2, 3\n",
    "        '''\n",
    "            action map:\n",
    "                UP    = 0\n",
    "                RIGHT = 1\n",
    "                DOWN  = 2\n",
    "                LEFT  = 3\n",
    "        '''\n",
    "       \n",
    "        MAX_Y = shape[0]\n",
    "        MAX_X = shape[1]\n",
    "\n",
    "        P = {} # empty dictionary for probabilities\n",
    "        grid = np.arange(self.num_states).reshape(shape)\n",
    "\n",
    "        itr = np.nditer(grid, flags=['multi_index'])\n",
    "\n",
    "        while not itr.finished:\n",
    "            state = itr.iterindex\n",
    "            y, x = itr.multi_index\n",
    "\n",
    "            '''\n",
    "                The probability dictionary is supposed to contain the probability of action with respect to the current state, next state, reward, and if the episode is done\n",
    "\n",
    "                So, as the equation, it looks like P[s][a] = (probability, next_state, rewards, is_done)\n",
    "\n",
    "            '''\n",
    "            P[state] = {actions: [] for actions in range (self.num_actions)}\n",
    "            is_done = lambda s: s == 0 or s == (self.num_states-1)\n",
    "            reward = 0.0 if is_done(state) else -1.0 \n",
    "\n",
    "            #checking if the agent is stuck in a terminal state. i.e the endpoints . this is for grid world example\n",
    "\n",
    "            if (is_done(state)):\n",
    "                for i in range (self.num_actions) :\n",
    "                    P[state][i] = [(1.0, state, reward, True)]  #Setting Probability for going in any direction\n",
    "            else:\n",
    "                next_state_up = state if (y == 0) else (state - MAX_X)\n",
    "                next_state_down = state if (y == MAX_Y - 1) else (state + MAX_X)\n",
    "                next_state_right = state if (x == MAX_X - 1) else (state + 1)\n",
    "                next_state_left = state if (x == 0) else (state - 1)\n",
    "\n",
    "                P[state][0] = [(1.0, next_state_up, reward, is_done(next_state_up))]\n",
    "                P[state][1] = [(1.0, next_state_right, reward, is_done(next_state_right))]\n",
    "                P[state][2] = [(1.0, next_state_down, reward, is_done(next_state_down))]\n",
    "                P[state][3] = [(1.0, next_state_left, reward, is_done(next_state_left))]\n",
    "\n",
    "            itr.iternext()\n",
    "        \n",
    "        #initial state distribution is supposed to be uniform\n",
    "        init_state_ds = np.ones(self.num_states) / self.num_states \n",
    "        \n",
    "        self.P = P \n",
    "\n",
    "        super(env, self).__init__()\n",
    "\n",
    "        def render(self, mode:str = 'human', close:bool = False):\n",
    "            if(close):\n",
    "                return\n",
    "            \n",
    "            outfile = io.StringIO() if mode == 'ansi' else sys.stdout\n",
    "\n",
    "            grid = np.arange(self.num_states).reshape(self.shape)\n",
    "\n",
    "            itr = np.nditer(grid, flags = ['multi_index'])\n",
    "            while not itr.finished:\n",
    "                s = itr.iterindex\n",
    "                y,x = itr.multi_index\n",
    "\n",
    "                if(self.s == s):\n",
    "                    output = \"x\" #current position/state\n",
    "                elif(s == 0 or s == (self.num_states - 1)):\n",
    "                    output = \"T\"\n",
    "                else:\n",
    "                    output = \"o\"\n",
    "\n",
    "                if(x == 0):\n",
    "                    output = output.lstrip()\n",
    "                \n",
    "                if (x == self.shape[1] - 1):\n",
    "                    output = output.rstrip()\n",
    "\n",
    "                outfile.write(output)\n",
    "                if(x == self.shape[1] -1 ):\n",
    "                    outfile.write(\"\\n\")\n",
    "\n",
    "                itr.iternext()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "669d25c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Env = env()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "687fc97e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def eval_policy(policy, env, discount_factor = 1.0, theta = 0.00001):\n",
    "\n",
    "    V = np.zeros(env.num_states)\n",
    "\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.num_states):\n",
    "            v = 0\n",
    "            for action, action_prob in enumerate(policy[s]):\n",
    "                for prob, next_state, reward, done in env.P[s][action]:\n",
    "                    v += action_prob * prob * (reward + discount_factor*V[next_state])\n",
    "\n",
    "            delta = max(delta, np.abs(v - V[s]))\n",
    "            V[s] = v\n",
    "            \n",
    "            if delta < theta:\n",
    "                break\n",
    "\n",
    "        return np.array(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1df66e93",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "random_policy = np.ones([Env.num_states, Env.num_actions])/Env.num_actions\n",
    "v = eval_policy(random_policy, Env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc16ee0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
